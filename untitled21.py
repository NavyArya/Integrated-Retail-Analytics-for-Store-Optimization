# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18tlo-EXshEicE2GrdnP_QQfNzokIP_oC

# PROJECT- Integrated Retail Analytics for Store Optimization

# PROJECT TYPE- The project type, based on your dataset names (stores, features, and sales) and supporting materials, is an integrated retail analytics and store optimization project. It focuses on sales forecasting, anomaly detection, customer segmentation, market basket analysis, and strategic business insights using real-world retail data.

Project Type Details
Domain: Retail analytics/store management

Goals: Sales trend analysis, forecasting, segmenting stores/customers, uncovering product affinities, and generating actionable recommendations.

Datasets Used: Store details (location, type, size), weekly sales transactions, and features/external factors (e.g., CPI, unemployment, fuel price).

Methods: Predictive modeling, clustering, market basket, time-series analysis, and business intelligence reporting.

This project type is common in retail and business intelligence scenarios and is designed for advanced data-driven decision making across inventory, marketing, and operational strategy.

# Contribution- Individual

# Project Summary-
Objective:

To analyze and optimize retail store operations by uncovering patterns in sales, understanding the impact of external factors, segmenting stores/customers, and generating actionable recommendations for inventory, marketing, and layout.

Datasets:

stores: Contains store-level information (size, type).

features: Tracks external factors (CPI, fuel price, unemployment, promotions).

sales: Records weekly sales transactions per department and store.

Techniques Used:

Data cleaning, anomaly detection, and feature engineering

Exploratory data analysis and statistical assessment

Time-series analysis (sales forecasting, trend decomposition)

Market basket analysis (apriori, association rules)

KMeans segmentation to identify store/customer groups

Model performance evaluation (RMSE, silhouette score, precision, recall)

Integration of macroeconomic and promotional effects

Generation of practical business/strategy insights

Key Deliverables:

Identification of slow-moving inventory for reduction or promotion

Assessment of marketing campaign performance via markdown/sales analysis

Segmentation of stores to prioritize operational changes and resource allocation

Forecasts of future sales to guide staffing, promotions, and inventory control

Reporting of actionable recommendations with feasibility assessment

Business Impact:

Enhances decision making with data-driven recommendations

Aligns operations and campaigns with real demand and store profiles

Facilitates targeted problem-solving for underperforming areas and products

# Github- https://github.com/NavyArya/Integrated-Retail-Analytics-for-Store-Optimization.git

# Problem Statement- Retail businesses face ongoing challenges in optimizing store operations, managing inventory, predicting demand, understanding customer behavior, and efficiently allocating marketing resources across large store networks. Traditional approaches often fail to adapt dynamically to changing sales trends, external factors (like economic conditions), and evolving consumer demand patterns.

The core problem:
How can a retail chain leverage its historical sales, store attributes, and external factors (such as fuel prices, unemployment, and CPI) to develop a data-driven system that:

Forecasts future sales at store and department levels,

Identifies anomalies and operational issues in sales transactions,

Segments stores (or customer groups) for targeted strategies,

Uncovers product affinities to improve cross-selling,

Quantifies the impact of macroeconomic and promotional factors on performance,

And generates actionable recommendations for inventory management, marketing, and store layout optimization in a scalable, reliable, and interpretable way?

The solution requires integrating advanced analytics and machine learning techniques to transform raw retail data into strategic insights and practical actions that drive profitability and operational efficiency.

# Let's Begin!
"""

# Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load Datasets
from google.colab import files
files = files.upload()

stores = pd.read_csv('/content/stores data-set (1).csv')
features = pd.read_csv('/content/Features data set (1).csv')
sales = pd.read_csv('/content/sales data-set (1).csv')

# Missing values Analysis
print("Missing values in stores:")
print(stores.isnull().sum())
print("Missing values in features:")
print(features.isnull().sum())
print("Missing values in sales:")
print(sales.isnull().sum())

# Missing Value Handling
# For stores: If missing, fill with mode or median
for col in stores.columns:
    if stores[col].isnull().sum() > 0:
        stores[col] = stores[col].fillna(stores[col].mode())

# For features: MarkDown columns fill with zero; Others with forward fill
markdown_cols = [col for col in features.columns if 'MarkDown' in col]
features[markdown_cols] = features[markdown_cols].fillna(0)
features['CPI'] = features['CPI'].fillna(method='ffill')
features['Unemployment'] = features['Unemployment'].fillna(method='ffill')
for col in features.columns:
    if features[col].isnull().sum() > 0:
        features[col] = features[col].fillna(features[col].mode())

# For sales: Weekly_Sales can't be missing. Remove rows if any.
sales = sales.dropna(subset=['Weekly_Sales'])

# Outlier Detection and Handling
# Example: Remove negative Weekly_Sales as outlier
sales = sales[sales['Weekly_Sales'] >= 0]

# Example: Z-score method for outlier removal in features' Temperature
from scipy.stats import zscore
features['temp_zscore'] = zscore(features['Temperature'])
features_clean = features[np.abs(features['temp_zscore']) < 3]  # Only normal temperatures
features_clean = features_clean.drop(columns=['temp_zscore'])

# Anomaly Detection
# Find anomalies in Store Size (e.g., stores unusually small/large)
plt.hist(stores['Size'], bins=20)
plt.title('Store Size Distribution')
plt.xlabel('Size')
plt.ylabel('Frequency')
plt.show()

# Print summary after cleaning
print("\nStores (after cleaning):")
print(stores.describe())
print("\nFeatures (after cleaning):")
print(features_clean.describe())
print("\nSales (after cleaning):")
print(sales.describe())

# Plot the distribution of recorded temperatures from the features dataset.
# This helps understand the environmental conditions across the dataset
# and can be useful in exploring seasonal effects on sales.
plt.figure(figsize=(10,5))
plt.hist(features['Temperature'], bins=30)
plt.title('Temperature Distribution')
plt.xlabel('Temperature')
plt.ylabel('Frequency')
plt.show()
# Plot the distribution of weekly sales from the sales dataset.
# This visualizes how sales figures are spread out, identifies skewness,
# and helps detect outliers or common sales ranges across departments/stores.
plt.figure(figsize=(10,5))
plt.hist(sales['Weekly_Sales'], bins=30)
plt.title('Weekly Sales Distribution')
plt.xlabel('Weekly Sales')
plt.ylabel('Frequency')
plt.show()

# Date Features for Temporal Analysis
features['Date'] = pd.to_datetime(features['Date'], format='%d/%m/%Y')
sales['Date'] = pd.to_datetime(sales['Date'], format='%d/%m/%Y')

# Extracting Year, Month, Week, Day Name
for df in [features, sales]:
    df['Year'] = df['Date'].dt.year
    df['Month'] = df['Date'].dt.month
    df['Week'] = df['Date'].dt.isocalendar().week
    df['DayOfWeek'] = df['Date'].dt.day_name()

# Season Feature (Winter, Spring, Summer, Fall)
def get_season(month):
    if month in [12, 1, 2]:
        return 'Winter'
    elif month in [3, 4, 5]:
        return 'Spring'
    elif month in [6, 7, 8]:
        return 'Summer'
    else:
        return 'Fall'

features['Season'] = features['Month'].apply(get_season)
sales['Season'] = sales['Month'].apply(get_season)


# MarkDown Activity Flag
markdown_cols = [col for col in features.columns if 'MarkDown' in col]
features['AnyMarkDown'] = (features[markdown_cols].sum(axis=1) > 0).astype(int)

# Store Size Buckets for Retail Segmentation
stores['Size_Bucket'] = pd.qcut(stores['Size'], q=3, labels=['Small', 'Medium', 'Large'])

# Merge for Comprehensive Feature Creation
merged = sales.merge(features, on=['Store','Date'], how='left').merge(stores, on='Store', how='left')

# Example: Holiday & Season Interaction Feature
merged['HolidaySeasonCombo'] = merged['IsHoliday_x'].astype(str) + '_' + merged['Season_y']

# Lagged Weekly Sales (to model momentum/trend)
merged = merged.sort_values(['Store', 'Dept', 'Date'])
merged['Weekly_Sales_Lag1'] = merged.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1)
merged['Weekly_Sales_Lag7'] = merged.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(7)

# Revenue per Sq.Ft. (Store Productivity)
merged['Revenue_per_SqFt'] = merged['Weekly_Sales'] / merged['Size']

# Print new feature samples
print(merged[['Date','Store','Dept','Weekly_Sales','Season_y','AnyMarkDown','Size_Bucket','HolidaySeasonCombo','Weekly_Sales_Lag1','Revenue_per_SqFt']].head())

# Basic Structure
print("Stores shape:", stores.shape)
print("Features shape:", features.shape)
print("Sales shape:", sales.shape)
print("\nStore columns:", stores.columns)
print("Features columns:", features.columns)
print("Sales columns:", sales.columns)

# Sample Records
print("\nStores sample:")
print(stores.head())
print("\nFeatures sample:")
print(features.head())
print("\nSales sample:")
print(sales.head())

# Statistical Summaries
print("\nStores statistics:")
print(stores.describe())
print("Store Type counts:\n", stores['Type'].value_counts())
print("\nFeatures statistics:")
print(features.describe())
print("Holiday count:", features['IsHoliday'].value_counts())
print("\nSales statistics:")
print(sales.describe())
print("Sales by Dept (sample):\n", sales.groupby('Dept')['Weekly_Sales'].mean().head())

# Distribution Plots (Visual Data Exploration)
plt.figure(figsize=(8,4))
sns.histplot(stores['Size'], bins=15)
plt.title('Store Size Distribution')

plt.figure(figsize=(8,4))
sns.histplot(features['Temperature'], bins=20)
plt.title('Temperature Distribution')

plt.figure(figsize=(8,4))
sns.histplot(sales['Weekly_Sales'], bins=30)
plt.title('Weekly Sales Distribution')

plt.show()

# Correlation Analysis (numeric columns)
numeric_features = features.select_dtypes(include='number')
plt.figure(figsize=(10,8))
sns.heatmap(numeric_features.corr(), annot=True, cmap='coolwarm')
plt.title('Features Correlation Matrix')
plt.show()

# Cross Tabulations (Relationships)
store_type_sales = sales.merge(stores, on='Store').groupby('Type')['Weekly_Sales'].mean()
print("\nAverage Weekly Sales by Store Type:\n", store_type_sales)
holiday_sales = sales.groupby('IsHoliday')['Weekly_Sales'].mean()
print("\nAverage Weekly Sales by Holiday Flag:\n", holiday_sales)

import pandas as pd

# Load datasets
stores = pd.read_csv('/content/stores data-set (1).csv')
features = pd.read_csv('/content/Features data set (1).csv')
sales = pd.read_csv('/content/sales data-set (1).csv')

# Format dates
features['Date'] = pd.to_datetime(features['Date'], format='%d/%m/%Y')
sales['Date'] = pd.to_datetime(sales['Date'], format='%d/%m/%Y')

# Merge all datasets
merged = sales.merge(features, on=['Store','Date'], how='left').merge(stores, on='Store', how='left')
print("Merged data shape:", merged.shape)
print(merged.head())  # Show first few integrated records

from sklearn.ensemble import IsolationForest

# Select numeric features
X_anomaly = merged[['Weekly_Sales', 'Temperature', 'Fuel_Price', 'Size']].fillna(0)

iso_model = IsolationForest(contamination=0.01, random_state=42)
merged['anomaly'] = iso_model.fit_predict(X_anomaly)

# Output: Print count and sample anomalies
num_anomalies = (merged['anomaly'] == -1).sum()
print("Number of sales anomalies detected:", num_anomalies)
print(merged.loc[merged['anomaly'] == -1,
                 ['Store', 'Dept', 'Date', 'Weekly_Sales', 'Temperature', 'Size']].head())

from sklearn.cluster import KMeans

# Prepare data for clustering: group by store
store_stats = merged.groupby('Store').agg({
    'Weekly_Sales': 'mean',
    'Size': 'first',
    'Type': 'first'
}).reset_index()

# Encode store type
type_dict = {'A':0, 'B':1, 'C':2}
store_stats['Type_enc'] = store_stats['Type'].map(type_dict)

X_cluster = store_stats[['Weekly_Sales', 'Size', 'Type_enc']].fillna(0)

# Run KMeans
kmeans_model = KMeans(n_clusters=3, random_state=42)
store_stats['Segment'] = kmeans_model.fit_predict(X_cluster)

# Output: Print segment assignments
print("Store segmentation output (first five rows):")
print(store_stats[['Store', 'Weekly_Sales', 'Size', 'Type', 'Segment']].head())

# Optional visualization
import matplotlib.pyplot as plt
plt.scatter(store_stats['Size'], store_stats['Weekly_Sales'], c=store_stats['Segment'], cmap='Set1', s=100)
plt.xlabel('Store Size')
plt.ylabel('Mean Weekly Sales')
plt.title('Store Segmentation')
plt.show()

from sklearn.linear_model import LinearRegression

# Select a single store & department for time series forecasting
dept_sample = merged['Dept'].unique()[0]
sample = merged[(merged['Store'] == 1) & (merged['Dept'] == dept_sample)].sort_values('Date')
sample['WeekOfYear'] = sample['Date'].dt.isocalendar().week

X_forecast = sample[['WeekOfYear']]
y_forecast = sample['Weekly_Sales']

# Fit Linear Regression
lr_model = LinearRegression()
lr_model.fit(X_forecast, y_forecast)
sample['Forecast_Sales'] = lr_model.predict(X_forecast)

# Output: Print actual vs predicted for first five
print("Actual vs Forecasted weekly sales (first five rows):")
print(sample[['Date','Weekly_Sales','Forecast_Sales']].head())

# Plot results
plt.plot(sample['Date'], sample['Weekly_Sales'], label='Actual Sales')
plt.plot(sample['Date'], sample['Forecast_Sales'], label='Forecast Sales', linestyle='--')
plt.xlabel('Date')
plt.ylabel('Weekly Sales')
plt.title('Demand Forecasting for Store 1, Dept {}'.format(dept_sample))
plt.legend()
plt.show()

import pandas as pd
from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error
from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib.pyplot as plt

# Load datasets
stores = pd.read_csv('/content/stores data-set (1).csv')
features = pd.read_csv('/content/Features data set (1).csv')
sales = pd.read_csv('/content/sales data-set (1).csv')

# Datetime conversion
features['Date'] = pd.to_datetime(features['Date'], format='%d/%m/%Y')
sales['Date'] = pd.to_datetime(sales['Date'], format='%d/%m/%Y')

# Merge datasets
merged = sales.merge(features, on=['Store', 'Date'], how='left').merge(stores, on='Store', how='left')

# Features for anomaly detection
X_anom = merged[['Weekly_Sales', 'Temperature', 'Fuel_Price', 'Size']].fillna(0)
iso = IsolationForest(contamination=0.01, random_state=42)
merged['anomaly_pred'] = iso.fit_predict(X_anom)

# Example: no true labels, so report count of anomalies detected
print("Number of anomalies detected:", (merged['anomaly_pred'] == -1).sum())

# If true anomaly labels exist (adjust names), uncomment below
# y_true = merged['anomaly_label']
# y_pred = merged['anomaly_pred'].replace({1: 0, -1: 1})
# print('Precision:', precision_score(y_true, y_pred))
# print('Recall:', recall_score(y_true, y_pred))
# print('F1 Score:', f1_score(y_true, y_pred))

# Prepare data for clustering
store_summary = merged.groupby('Store').agg({'Weekly_Sales': 'mean', 'Size': 'first', 'Type': 'first'}).reset_index()
store_summary['Type_enc'] = store_summary['Type'].map({'A': 0, 'B': 1, 'C': 2})
X_cluster = store_summary[['Weekly_Sales', 'Size', 'Type_enc']].fillna(0)

# KMeans clustering
kmeans = KMeans(n_clusters=3, random_state=42)
store_summary['Segment'] = kmeans.fit_predict(X_cluster)

# Calculate Silhouette Score
sil_score = silhouette_score(X_cluster, store_summary['Segment'])
print("Silhouette Score for KMeans clustering:", sil_score)

# Select one store and dept for forecasting
sample = merged[(merged['Store'] == 1) & (merged['Dept'] == merged['Dept'].unique()[0])].sort_values('Date')
sample['Week'] = sample['Date'].dt.isocalendar().week

X = sample[['Week']]
y = sample['Weekly_Sales']

lr = LinearRegression()
lr.fit(X, y)
sample['Forecast'] = lr.predict(X)

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(y, sample['Forecast']))
print("Demand Forecasting RMSE:", rmse)

# Plot actual vs predicted
plt.plot(sample['Date'], y, label='Actual Sales')
plt.plot(sample['Date'], sample['Forecast'], label='Forecast', linestyle='--')
plt.legend()
plt.title('Weekly Sales Forecast vs Actual')
plt.xlabel('Date')
plt.ylabel('Weekly Sales')
plt.show()

# Load datasets
stores = pd.read_csv('/content/stores data-set (1).csv')
features = pd.read_csv('/content/Features data set (1).csv')
sales = pd.read_csv('/content/sales data-set (1).csv')

# Convert 'Date' to datetime
sales['Date'] = pd.to_datetime(sales['Date'], format='%d/%m/%Y')

# Aggregate weekly sales by date (all stores and departments combined)
weekly_sales = sales.groupby('Date')['Weekly_Sales'].sum().reset_index()
weekly_sales.set_index('Date', inplace=True)

print("Weekly sales timeseries head:")
print(weekly_sales.head())

import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose

# Decompose weekly sales (additive model)
decomposition = seasonal_decompose(weekly_sales['Weekly_Sales'], model='additive', period=52)  # yearly seasonality approx

# Plot decomposition components
decomposition.plot()
plt.suptitle('Time-Series Decomposition of Weekly Sales')
plt.show()

from statsmodels.tsa.arima.model import ARIMA
import numpy as np

# Fit ARIMA model (parameters p,d,q can be tuned)
model = ARIMA(weekly_sales['Weekly_Sales'], order=(2,1,2))
model_fit = model.fit()

# Summary of the model
print(model_fit.summary())

# Forecast next 12 weeks
forecast_steps = 12
forecast = model_fit.forecast(steps=forecast_steps)
print("12-week Sales Forecast:")
print(forecast)

# Plot actual + forecast
plt.figure(figsize=(12,5))
plt.plot(weekly_sales.index, weekly_sales['Weekly_Sales'], label='Historical Sales')
plt.plot(pd.date_range(weekly_sales.index[-1], periods=forecast_steps+1, freq='W')[1:], forecast, label='Forecast', marker='o')
plt.title('Weekly Sales Forecast (ARIMA)')
plt.xlabel('Date')
plt.ylabel('Weekly Sales')
plt.legend()
plt.show()

import pandas as pd

# Load sales data
sales = pd.read_csv('/content/sales data-set (1).csv')

# Prepare transaction data (basket of products per store & date)
# Assuming Dept represents product category/department sold
basket_data = sales.groupby(['Store', 'Date', 'Dept'])['Weekly_Sales'].sum().reset_index()

# Convert sales > 0 as purchased (1), else 0
basket_data['Purchased'] = basket_data['Weekly_Sales'].apply(lambda x: 1 if x > 0 else 0)

# Pivot to create basket format: rows=Store+Date, cols=Dept, values=Purchased
basket = basket_data.pivot_table(index=['Store', 'Date'], columns='Dept', values='Purchased', fill_value=0)

print(basket.head())

import pandas as pd

# Load sales data
sales = pd.read_csv('/content/sales data-set (1).csv')

# Prepare transaction data (basket of products per store & date)
# Assuming Dept represents product category/department sold
basket_data = sales.groupby(['Store', 'Date', 'Dept'])['Weekly_Sales'].sum().reset_index()

# Convert sales > 0 as purchased (1), else 0
basket_data['Purchased'] = basket_data['Weekly_Sales'].apply(lambda x: 1 if x > 0 else 0)

# Pivot to create basket format: rows=Store+Date, cols=Dept, values=Purchased
basket = basket_data.pivot_table(index=['Store', 'Date'], columns='Dept', values='Purchased', fill_value=0)

print(basket.head())

import pandas as pd

# Load datasets
stores = pd.read_csv('/content/stores data-set (1).csv')
features = pd.read_csv('/content/Features data set (1).csv')
sales = pd.read_csv('/content/sales data-set (1).csv')

# Merge all datasets to get store-level sales aggregates
features['Date'] = pd.to_datetime(features['Date'], format='%d/%m/%Y')
sales['Date'] = pd.to_datetime(sales['Date'], format='%d/%m/%Y')
merged = sales.merge(features, on=['Store', 'Date'], how='left').merge(stores, on='Store', how='left')

# Calculate average weekly sales per store
store_sales = merged.groupby('Store').agg({
    'Weekly_Sales':'mean',
    'Size':'first',
    'Type':'first'
}).reset_index()

# Encode categorical Type to numeric
store_sales['Type_enc'] = store_sales['Type'].map({'A': 0, 'B': 1, 'C': 2})

print(store_sales.head())

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Features for clustering
X = store_sales[['Weekly_Sales', 'Size', 'Type_enc']]

# Scale features for better clustering
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# KMeans with 3 clusters (tune n_clusters as needed)
kmeans = KMeans(n_clusters=3, random_state=42)
store_sales['Segment'] = kmeans.fit_predict(X_scaled)

print(store_sales[['Store', 'Weekly_Sales', 'Size', 'Type', 'Segment']].head())

from sklearn.metrics import silhouette_score

# Silhouette score for cluster quality (range -1 to 1, higher is better)
score = silhouette_score(X_scaled, store_sales['Segment'])
print(f'Silhouette Score: {score:.3f}')

# Segment characteristics
segment_summary = store_sales.groupby('Segment').agg({
    'Weekly_Sales':'mean',
    'Size':'mean',
    'Type_enc':'mean',
    'Store':'count'
}).rename(columns={'Store':'Count'})

print("Segment summary statistics:")
print(segment_summary)

import pandas as pd

# Load datasets
stores = pd.read_csv('/content/stores data-set (1).csv')
features = pd.read_csv('/content/Features data set (1).csv')
sales = pd.read_csv('/content/sales data-set (1).csv')

# Convert Date columns to datetime
features['Date'] = pd.to_datetime(features['Date'], format='%d/%m/%Y')
sales['Date'] = pd.to_datetime(sales['Date'], format='%d/%m/%Y')

# Merge datasets on Store and Date to bring external factors with sales & store info
data = sales.merge(features[['Store', 'Date', 'CPI', 'Unemployment', 'Fuel_Price']], on=['Store', 'Date'], how='left')
data = data.merge(stores[['Store', 'Type', 'Size']], on='Store', how='left')

print("Merged data sample with external factors:")
print(data[['Store', 'Date', 'Weekly_Sales', 'CPI', 'Unemployment', 'Fuel_Price', 'Type', 'Size']].head())

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np

# Prepare features and target (including external factors)
X = data[['CPI', 'Unemployment', 'Fuel_Price', 'Size']]
y = data['Weekly_Sales']

# Drop rows with missing sales or features
mask = X.notnull().all(axis=1) & y.notnull()
X, y = X[mask], y[mask]

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Fit model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"Model RMSE including external factors: {rmse:.2f}")

# Show sample predictions vs actual
df_results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}).reset_index(drop=True)
print(df_results.head())

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load datasets & merge external factors
stores = pd.read_csv('/content/stores data-set (1).csv')
features = pd.read_csv('/content/Features data set (1).csv')
sales = pd.read_csv('/content/sales data-set (1).csv')

features['Date'] = pd.to_datetime(features['Date'], format='%d/%m/%Y')
sales['Date'] = pd.to_datetime(sales['Date'], format='%d/%m/%Y')

data = sales.merge(features[['Store', 'Date', 'CPI', 'Unemployment', 'Fuel_Price']], on=['Store', 'Date'], how='left')
data = data.merge(stores[['Store', 'Size']], on='Store', how='left')

X = data[['CPI', 'Unemployment', 'Fuel_Price', 'Size']].fillna(method='ffill')
y = data['Weekly_Sales'].fillna(method='ffill')

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model fitting
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Test RMSE: {rmse:.2f}")

# Model coefficients interpretation
coef_df = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': model.coef_
})
print("Model Coefficients:")
print(coef_df)

# Pairplot of features to check relationships with Weekly_Sales
data_sample = data[['Weekly_Sales', 'CPI', 'Unemployment', 'Fuel_Price', 'Size']].dropna().sample(1000, random_state=42)
sns.pairplot(data_sample)
plt.suptitle('Feature Relationships with Weekly Sales', y=1.02)
plt.show()

# Plot predicted vs actual
plt.figure(figsize=(8,5))
plt.scatter(y_test, y_pred, alpha=0.3)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red')
plt.xlabel('Actual Weekly Sales')
plt.ylabel('Predicted Weekly Sales')
plt.title('Actual vs Predicted Sales')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming you have a fitted model and data X, y, and predictions y_pred from earlier steps

# Display coefficients with direction and magnitude
coef_df = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': model.coef_
}).sort_values(by='Coefficient', ascending=False)

print("Feature impact on Weekly Sales (Coefficient values):")
print(coef_df)

# Bar plot for easy visual interpretation of coefficients
plt.figure(figsize=(8,5))
sns.barplot(x='Coefficient', y='Feature', data=coef_df, palette='viridis')
plt.title('Impact of External Factors on Weekly Sales')
plt.xlabel('Coefficient Value')
plt.ylabel('Feature')
plt.show()

# Scatter plots with regression lines to visualize relationships
for feature in X.columns:
    plt.figure(figsize=(6,4))
    sns.regplot(x=data[feature], y=data['Weekly_Sales'], scatter_kws={'alpha':0.3})
    plt.title(f'Relationship Between {feature} and Weekly Sales')
    plt.xlabel(feature)
    plt.ylabel('Weekly Sales')
    plt.show()

# Plot actual vs predicted sales with diagonal reference
plt.figure(figsize=(8,6))
plt.scatter(y_test, y_pred, alpha=0.4)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', lw=2)
plt.title('Actual vs Predicted Weekly Sales')
plt.xlabel('Actual Sales')
plt.ylabel('Predicted Sales')
plt.show()

import pandas as pd

stores = pd.read_csv('/content/stores data-set (1).csv')
features = pd.read_csv('/content/Features data set (1).csv')
sales = pd.read_csv('/content/sales data-set (1).csv')

features['Date'] = pd.to_datetime(features['Date'], format='%d/%m/%Y')
sales['Date'] = pd.to_datetime(sales['Date'], format='%d/%m/%Y')

merged = sales.merge(features, on=['Store','Date'], how='left').merge(stores, on='Store', how='left')

# Identify slow-moving products: low average weekly sales
slow_products = merged.groupby('Dept')['Weekly_Sales'].mean().sort_values().head(10)
print("Slow-moving departments/products (low avg weekly sales):")
print(slow_products)

# Products with most markdowns (potential promotion overuse or opportunity)
markdown_cols = [col for col in features.columns if 'MarkDown' in col]
merged['TotalMarkdown'] = merged[markdown_cols].sum(axis=1)
markdown_effect = merged.groupby('Dept').agg({'TotalMarkdown':'sum', 'Weekly_Sales':'mean'})
markdown_effect = markdown_effect.sort_values(by='TotalMarkdown', ascending=False).head(10)
print("\nDepartments with highest markdown spend:")
print(markdown_effect)

# Compare sales on holidays vs non-holidays
holiday_sales = merged.groupby('IsHoliday_x')['Weekly_Sales'].mean()
print("\nAverage Weekly Sales by Holiday Status:")
print(holiday_sales)

# Effectiveness of markdowns on sales (Group sales by markdown usage)
merged['HasMarkdown'] = merged['TotalMarkdown'] > 0
markdown_sales = merged.groupby('HasMarkdown')['Weekly_Sales'].mean()
print("\nAverage sales by markdown usage (0=no,1=yes):")
print(markdown_sales)

# Segment stores by size and average sales to identify underperformers
store_perf = merged.groupby('Store').agg({'Weekly_Sales':'mean', 'Size':'first', 'Type':'first'})
store_perf['Performance'] = store_perf['Weekly_Sales'] / store_perf['Size']  # Sales per sq ft

# Highlight stores with below average sales per sq ft
avg_perf = store_perf['Performance'].mean()
underperformers = store_perf[store_perf['Performance'] < avg_perf]
print("\nUnderperforming stores (low sales per sq ft):")
print(underperformers[['Weekly_Sales', 'Size', 'Performance']])

# Output stores likely needing optimization
print(f"\nAverage sales per sq ft across stores: {avg_perf:.2f}")

import pandas as pd

# Load datasets
stores = pd.read_csv('/content/stores data-set (1).csv')
features = pd.read_csv('/content/Features data set (1).csv')
sales = pd.read_csv('/content/sales data-set (1).csv')

features['Date'] = pd.to_datetime(features['Date'], format='%d/%m/%Y')
sales['Date'] = pd.to_datetime(sales['Date'], format='%d/%m/%Y')

# Merge for comprehensive analysis
merged = sales.merge(features, on=['Store', 'Date'], how='left').merge(stores, on='Store', how='left')

# Baseline average weekly sales per store and dept
baseline_sales = merged.groupby(['Store', 'Dept']).agg(
    avg_weekly_sales=('Weekly_Sales', 'mean'),
    sales_count=('Weekly_Sales', 'count')
).reset_index()

print("Baseline average weekly sales:")
print(baseline_sales.head())

# Identify slow movers (low avg sales)
slow_movers = baseline_sales[baseline_sales['avg_weekly_sales'] < 5000]  # Threshold adjustable

# Simulate 20% stock reduction impact on inventory cost
average_stock_cost_per_unit = 10  # Example cost per unit
slow_movers['estimated_stock_saving'] = slow_movers['avg_weekly_sales'] * 0.2 * average_stock_cost_per_unit

print("Potential stock cost savings on slow movers (sample):")
print(slow_movers[['Store', 'Dept', 'estimated_stock_saving']].head())

# Sales lift estimated from markdown usage (assumed 10% lift)
markdown_cols = [c for c in features.columns if 'MarkDown' in c]
merged['TotalMarkdown'] = merged[markdown_cols].sum(axis=1)
merged['HasMarkdown'] = merged['TotalMarkdown'] > 0

sales_lift = merged.groupby('HasMarkdown')['Weekly_Sales'].mean()
lift_percent = ((sales_lift[True] - sales_lift[False]) / sales_lift[False]) * 100

print(f"Estimated sales lift due to markdowns: {lift_percent:.2f}%")

# Estimate marketing budget impact assuming markdown spending
total_markdown_spent = merged['TotalMarkdown'].sum()
print(f"Total markdown spending impacting sales: ${total_markdown_spent:,.2f}")

# Sales per square foot to prioritize store focus
store_perf = merged.groupby('Store').agg(
    avg_weekly_sales=('Weekly_Sales', 'mean'),
    size=('Size', 'first')
).reset_index()

store_perf['sales_per_sqft'] = store_perf['avg_weekly_sales'] / store_perf['size']

# Target lowest quartile stores for optimization
threshold = store_perf['sales_per_sqft'].quantile(0.25)
stores_to_optimize = store_perf[store_perf['sales_per_sqft'] < threshold]

print("Stores targeted for optimization (low sales/sqft):")
print(stores_to_optimize)

# Feasibility: Estimate potential sales boost of 15% post optimization
stores_to_optimize['expected_sales_increase'] = stores_to_optimize['avg_weekly_sales'] * 0.15
print("Estimated weekly sales increase from store optimization:")
print(stores_to_optimize[['Store', 'expected_sales_increase']])

# Interpretation and Real-World Applicability

# Inventory Management Interpretation
print("Inventory Management:")
print("- Slow-moving products identified signal opportunities for stock reduction or clearance sales.")
print("- Estimated stock cost savings demonstrate tangible financial benefits from inventory optimization.")

# Marketing Interpretation
print("\nMarketing:")
print("- Sales lift percentage due to markdowns quantifies the effectiveness of promotional activities.")
print("- Total markdown spend highlights the budget impact, enabling ROI analysis for marketing spend.")

# Store Optimization Interpretation
print("\nStore Optimization:")
print("- Stores with low sales per square foot flagged for operational improvements.")
print("- Estimated sales increase projections help prioritize resource allocation to maximize returns.")

print("\nOverall, these metrics provide actionable, financially grounded insights, demonstrating strategy feasibility and expected real-world impact.")

# Imports
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Constants and File Paths
STORES_FILE = '/content/stores data-set (1).csv'
FEATURES_FILE = '/content/Features data set (1).csv'
SALES_FILE = '/content/sales data-set (1).csv'

# Function to load datasets
def load_data(stores_fp, features_fp, sales_fp):
    stores = pd.read_csv(stores_fp)
    features = pd.read_csv(features_fp)
    sales = pd.read_csv(sales_fp)
    features['Date'] = pd.to_datetime(features['Date'], format='%d/%m/%Y')
    sales['Date'] = pd.to_datetime(sales['Date'], format='%d/%m/%Y')
    return stores, features, sales

# Function for data summary
def print_data_summary(stores, features, sales):
    print("Stores Data Summary:")
    print(stores.info())
    print(stores.describe())
    print("\nFeatures Data Summary:")
    print(features.info())
    print(features.describe())
    print("\nSales Data Summary:")
    print(sales.info())
    print(sales.describe())

# Function for data merge
def merge_data(stores, features, sales):
    merged = sales.merge(features, on=['Store', 'Date'], how='left').merge(stores, on='Store', how='left')
    return merged

# Function for basic visualizations
def plot_distributions(merged):
    plt.figure(figsize=(12,5))
    sns.histplot(merged['Weekly_Sales'].dropna(), bins=30, kde=True)
    plt.title('Weekly Sales Distribution')
    plt.xlabel('Weekly Sales')
    plt.show()

    plt.figure(figsize=(12,5))
    sns.histplot(merged['Size'].dropna(), bins=20, kde=True, color='orange')
    plt.title('Store Size Distribution')
    plt.xlabel('Store Size')
    plt.show()

    plt.figure(figsize=(12,5))
    sns.boxplot(x='Type', y='Weekly_Sales', data=merged)
    plt.title('Weekly Sales by Store Type')
    plt.show()

# Main workflow
def main():
    stores, features, sales = load_data(STORES_FILE, FEATURES_FILE, SALES_FILE)

    print_data_summary(stores, features, sales)

    merged = merge_data(stores, features, sales)

    print("\nMerged data sample:")
    print(merged.head())

    plot_distributions(merged)

if __name__ == '__main__':
    main()

# Import necessary libraries for data manipulation and visualization
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Define file paths for datasets to easily manage data location
STORES_FILE = '/content/stores data-set (1).csv'
FEATURES_FILE = '/content/Features data set (1).csv'
SALES_FILE = '/content/sales data-set (1).csv'

def load_data(stores_fp, features_fp, sales_fp):
    """
    Loads the three datasets from provided file paths.
    Converts Date columns in features and sales to datetime format for further analysis.

    Parameters:
        stores_fp (str): File path for stores dataset.
        features_fp (str): File path for features dataset.
        sales_fp (str): File path for sales dataset.

    Returns:
        stores (DataFrame): Stores information.
        features (DataFrame): Features including external factors.
        sales (DataFrame): Sales transaction data.
    """
    stores = pd.read_csv(stores_fp)
    features = pd.read_csv(features_fp)
    sales = pd.read_csv(sales_fp)

    features['Date'] = pd.to_datetime(features['Date'], format='%d/%m/%Y')
    sales['Date'] = pd.to_datetime(sales['Date'], format='%d/%m/%Y')

    return stores, features, sales

def print_data_summary(stores, features, sales):
    """
    Prints summary information and descriptive statistics for each dataset.
    Helps quickly understand the dataset structure, missing values, and distribution characteristics.

    Parameters:
        stores (DataFrame): Stores information.
        features (DataFrame): Features dataset.
        sales (DataFrame): Sales transaction data.
    """
    print("Stores Data Overview:")
    print(stores.info())
    print(stores.describe())
    print("\nFeatures Data Overview:")
    print(features.info())
    print(features.describe())
    print("\nSales Data Overview:")
    print(sales.info())
    print(sales.describe())

def merge_data(stores, features, sales):
    """
    Merges the stores, features, and sales datasets into a single DataFrame for integrated analysis.
    The merge is done on 'Store' and 'Date' as keys.

    Parameters:
        stores (DataFrame): Stores information.
        features (DataFrame): Features dataset.
        sales (DataFrame): Sales transaction data.

    Returns:
        merged (DataFrame): Consolidated dataset combining all information.
    """
    merged = sales.merge(features, on=['Store', 'Date'], how='left').merge(stores, on='Store', how='left')
    return merged

def plot_distributions(merged):
    """
    Generates basic plots to visualize distributions of key variables:
    - Weekly Sales distribution to understand sales range and skewness.
    - Store Size distribution to check store size variability.
    - Sales by store Type to identify differences among store categories.

    Parameters:
        merged (DataFrame): Combined dataset including sales and store info.
    """
    plt.figure(figsize=(12,5))
    sns.histplot(merged['Weekly_Sales'].dropna(), bins=30, kde=True)
    plt.title('Weekly Sales Distribution')
    plt.xlabel('Weekly Sales')
    plt.show()

    plt.figure(figsize=(12,5))
    sns.histplot(merged['Size'].dropna(), bins=20, kde=True, color='orange')
    plt.title('Store Size Distribution')
    plt.xlabel('Store Size')
    plt.show()

    plt.figure(figsize=(12,5))
    sns.boxplot(x='Type', y='Weekly_Sales', data=merged)
    plt.title('Weekly Sales by Store Type')
    plt.show()

def main():
    """
    Main function orchestrates the data loading, exploration, merging, and visualization process.
    It is intended to provide a clean and easy-to-follow workflow.
    """
    stores, features, sales = load_data(STORES_FILE, FEATURES_FILE, SALES_FILE)

    print_data_summary(stores, features, sales)

    merged = merge_data(stores, features, sales)

    print("\nSample of Merged Data:")
    print(merged.head())

    plot_distributions(merged)

# Execute main workflow
if __name__ == '__main__':
    main()